# 05.4 - Testing & Change Management

## Overview
Deploying changes to production Power BI reports without systematic testing creates risk of breaking clinical workflows, introducing calculation errors, or causing performance degradation. In healthcare environments where providers rely on dashboards for patient care decisions (daily census, quality metrics, readmission tracking), untested changes can lead to incorrect clinical decisions or workflow disruption. A structured testing and change management process ensures that updates are validated in Test environments with realistic data volumes and user scenarios before reaching Production, minimizing downtime and maintaining stakeholder confidence. This includes user acceptance testing (UAT), regression testing, change communication protocols, and rollback procedures for when deployments go wrong.

## Key Principles

- **Test in Non-Production First**: Never test changes directly in Production workspace. Use Dev workspace for active development, Test workspace for UAT with production-like data, and only promote to Production after stakeholder sign-off. This prevents users from seeing half-finished features or broken calculations.
- **UAT with Real Users and Scenarios**: Conduct user acceptance testing with actual clinical end-users (providers, care coordinators, analysts) using realistic workflows. Don't rely solely on developer testingusers uncover edge cases and usability issues that developers miss. Provide UAT checklist with specific scenarios to validate.
- **Regression Testing for Changes**: When modifying DAX measures, relationships, or data model structure, test all related reports and visuals to ensure changes don't break existing functionality. A seemingly simple measure update can cascade failures if other measures depend on it (measure chaining).
- **Change Communication and Documentation**: Inform stakeholders of upcoming changes before deployment, including what's changing, why, expected impact, and deployment timeline. Use change notification templates for consistency. Document changes in release notes for audit trail and knowledge transfer.
- **Rollback Plan for Production Issues**: Always have a rollback strategy before deploying to Production. This can be as simple as keeping previous .pbix version in Dev workspace, exporting before deployment, or using deployment pipeline's "rollback" feature. Define maximum acceptable downtime (e.g., 1 hour) and rollback trigger criteria (e.g., >3 user-reported issues).

## Practical Examples

### Example 1: UAT Process for Daily Huddle Dashboard Update

**Scenario**: Development team updated daily huddle dashboard with new "High-Risk Patient" flag based on complex risk stratification logic. Before deploying to Production (used in 8:00 AM clinical huddles), conduct UAT to validate accuracy and usability.

**UAT Process**:

**1. UAT Preparation** (Developer):
- Deploy updated dashboard to Test workspace
- Update Test workspace data source to point to test database (recent production copy, not live data)
- Create UAT test plan document with specific scenarios

**2. UAT Test Plan** (scenarios to validate):
```
Daily Huddle Dashboard UAT - Version 2.3 - October 21, 2025

Scope: New "High-Risk Patient" flag and associated filtering

Test Scenarios:
¡ Scenario 1: Verify high-risk flag appears for patients with risk score e75
   - Navigate to "Patient Census" page
   - Apply filter: Risk Score e 75
   - Confirm all patients show red "High Risk" flag
   - Expected: 23 high-risk patients in test data

¡ Scenario 2: Verify high-risk flag does NOT appear for low-risk patients
   - Apply filter: Risk Score < 50
   - Confirm NO patients show "High Risk" flag
   - Expected: All patients show green "Low Risk" flag

¡ Scenario 3: Drillthrough to high-risk patient detail
   - Right-click high-risk patient row
   - Select "Drillthrough to Patient Detail"
   - Confirm patient detail page shows risk factors (recent ED visits, chronic conditions)
   - Expected: Detail page loads in <2 seconds

¡ Scenario 4: Filter high-risk patients by facility
   - Select "Memorial Hospital" from facility slicer
   - Confirm high-risk count updates (should show 8 patients)
   - Print to PDF, verify layout fits on one page
   - Expected: PDF printable for huddle meeting

¡ Scenario 5: Performance test with full production data volume
   - Remove all filters (show all patients across all facilities)
   - Measure page load time (Performance Analyzer)
   - Expected: <3 seconds initial load, <5 seconds with all visuals

¡ Scenario 6: Regression test - existing filters still work
   - Test date range slicer (last 7 days, last 30 days)
   - Test provider slicer
   - Test facility slicer
   - Expected: All existing slicers function without errors
```

**3. UAT Execution** (Clinical Users):
- Invite 3-4 representative users: 1 care coordinator, 1 provider, 1 analyst, 1 manager
- Schedule 30-minute UAT session in Test workspace
- Users execute test scenarios, document pass/fail and issues in shared document
- Developer observes but doesn't lead (let users discover issues organically)

**4. UAT Results Documentation**:
```
UAT Session Results - October 21, 2025, 2:00 PM

Participants: Dr. Sarah Johnson (provider), Maria Garcia (care coordinator),
              Tom Chen (analyst), Lisa Park (manager)

Test Results:
 Scenario 1: PASS - High-risk flags display correctly
 Scenario 2: PASS - Low-risk patients correctly flagged
 Scenario 3: PASS - Drillthrough works, loads in 1.2 seconds
 Scenario 4: FAIL - PDF layout cuts off right column (needs adjustment)
 Scenario 5: PASS - Page load 2.8 seconds (within target)
 Scenario 6: PASS - Regression tests passed

Issues Found:
1. PDF print layout cuts off "Assigned Care Coordinator" column (needs margin fix)
2. Tooltip on risk score gauge shows decimal places (should round to integer)
3. Request: Add "Export High-Risk List" button for care coordinators

UAT Decision: CONDITIONAL APPROVAL
- Fix Issue #1 (print layout) before Production deployment (required)
- Fix Issue #2 (tooltip formatting) before deployment (nice-to-have)
- Issue #3 (export button) deferred to next release (feature request, not blocker)

Next Steps: Developer fixes Issues #1 and #2, re-test print layout on Friday,
            deploy to Production on Monday morning (before 8:00 AM huddle)
```

**5. Post-UAT Actions**:
- Developer fixes identified issues in Dev workspace
- Re-test print layout in Test workspace (spot check, not full UAT)
- Update release notes with UAT feedback
- Schedule Production deployment for low-usage window (early Monday morning)

**Why this works**: Real clinical users test with realistic scenarios before Production deployment. Issues caught in UAT (print layout, tooltip formatting) would have disrupted Monday morning huddles if deployed untested. Conditional approval process prevents rushed deployments while maintaining momentum.

### Example 2: Regression Testing After Data Model Change

**Scenario**: Developer adds a new dimension table (DimPayer) to improve payer-level reporting. Need to ensure existing reports still function correctly after model change.

**Regression Test Checklist**:
```
Data Model Change: Added DimPayer dimension table
Potential Impact: Relationships, existing payer-related measures, report filters

Regression Test Areas:
¡ Visual Validation Tests:
  ¡ Financial dashboard still displays revenue by payer
  ¡ Member demographics page shows payer distribution
  ¡ Quality metrics page filters by payer correctly
  ¡ No error messages appear on any existing pages

¡ Calculation Validation Tests:
  ¡ Total Members measure matches pre-change value (15,847)
  ¡ Revenue by Payer measure matches pre-change totals ($12.4M)
  ¡ Member count by payer sums to total member count

¡ Relationship Validation Tests:
  ¡ FactEncounter ’ DimPayer relationship is 1:Many (not Many:Many)
  ¡ Cross-filter direction is single (Payer filters Encounters, not bidirectional)
  ¡ Row-level security still applies (providers don't see other providers' payers)

¡ Performance Validation Tests:
  ¡ Run Performance Analyzer on Financial Dashboard (before vs. after)
  ¡ Verify no DAX query time regressions (>20% slower = investigate)
  ¡ Check model size (new dimension should add <5 MB)

¡ Filter Validation Tests:
  ¡ Payer slicer filters all visuals correctly
  ¡ Clearing payer filter restores all data
  ¡ Payer filter combined with facility filter shows correct intersections
```

**Testing Workflow**:
1. **Before Change**: Export current Production .pbix to Dev workspace, run Performance Analyzer baseline, document current metric values
2. **Implement Change**: Add DimPayer table, create relationships, update measures in Dev workspace
3. **Test in Dev**: Execute regression test checklist, compare to baseline values
4. **Deploy to Test**: Publish to Test workspace, re-run regression tests with production-like data
5. **UAT**: Clinical users validate payer reporting accuracy with known test cases
6. **Deploy to Prod**: After sign-off, deploy during low-usage window (weekend evening)

**Automated Regression Testing** (advanced - using DAX Studio):
```dax
// Query to validate measure values haven't changed
EVALUATE
SUMMARIZECOLUMNS(
    "Total Members", [Total Members],
    "Total Revenue", [Total Revenue],
    "Active Providers", [Active Providers],
    "Model Size MB", DIVIDE(INFO.SIZE(), 1048576, 0)
)
```
Run this query before and after changes, compare results. Discrepancies indicate regression issues.

**Why this works**: Systematic regression testing catches unintended consequences of model changes. For example, adding DimPayer might accidentally introduce many-to-many relationships if junction table design is incorrect, causing inflated counts. Testing with known baseline values validates accuracy.

### Example 3: Change Communication Template

**Scenario**: Deploying new incremental refresh configuration to production patient census dashboard to improve refresh speed from 6 hours to 15 minutes. Communicate change to stakeholders in advance.

**Change Notification Email**:
```
To: Clinical Dashboard Users (All Staff)
From: Power BI Development Team
Subject: [Scheduled Maintenance] Patient Census Dashboard Update - Monday, Oct 23, 6:00 AM

Dear Clinical Team,

We will be deploying an update to the Patient Census Dashboard on Monday, October 23, 2025,
from 6:00 AM - 6:30 AM EST. This change will improve data refresh performance and provide
more up-to-date patient information throughout the day.

WHAT'S CHANGING:
- Data refresh strategy upgraded to incremental refresh (technical improvement)
- Census data will update every 2 hours instead of overnight-only
- No changes to dashboard visuals, layout, or user experience
- Report URL remains the same (no action required by users)

WHY THIS MATTERS:
- Current state: Census data updates once daily at 2:00 AM
- Future state: Census data updates every 2 hours (6:00 AM, 8:00 AM, 10:00 AM, etc.)
- Benefit: More current patient information for afternoon/evening huddles

EXPECTED DOWNTIME:
- Dashboard unavailable: 6:00 AM - 6:30 AM (30 minutes maximum)
- First updated data refresh: 8:00 AM (as usual for morning huddles)
- If you access the dashboard between 6:00-6:30 AM, you may see "Can't load report"
  message - please wait 5 minutes and refresh your browser

TESTING:
- Change tested in Test workspace with clinical staff on October 20
- Performance validated: Page load time remains <3 seconds
- User acceptance testing completed with no issues

ROLLBACK PLAN:
- If issues occur, we will revert to previous version within 1 hour
- Previous version available as backup in Production workspace
- Development team monitoring dashboard performance 6:30 AM - 12:00 PM

QUESTIONS OR CONCERNS:
- Contact: powerbi-support@absolutecare.com or IT Help Desk (ext. 5555)
- If you encounter issues after 6:30 AM, please report immediately

RELEASE NOTES:
- Version 3.2.0 ’ 3.3.0
- Feature: Incremental refresh enabled (2-hour refresh cycles)
- Performance: Refresh time reduced from 6 hours to 15 minutes
- Bug fixes: None (performance improvement only)

Thank you for your patience during this brief maintenance window.

Best regards,
Power BI Development Team
```

**Why this works**: Transparent communication sets expectations, explains "why" (not just "what"), provides specific downtime window, and offers support contact. Users aren't surprised by downtime, and questions are preemptively answered. Rollback plan demonstrates preparedness for issues.

## Common Pitfalls

### L Pitfall 1: Testing Only in Dev with Small Data Volumes
**Description**: Developers test changes in Dev workspace using small test datasets (1,000 rows) or "Sample Data" mode, then deploy directly to Production where real data has 2-5 million rows.

**Impact**: Changes that work fine with 1,000 rows cause timeouts or performance degradation with production data volumes. For example, a calculated column that takes 2 seconds with test data might take 15 minutes with 5M rows, blocking report refresh. Users experience slow dashboards or refresh failures after deployment.

**Prevention**:
- **Test Workspace Strategy**: Point Test workspace to database with production-like data volumes (full copy or recent subset)
- **Performance Testing**: Run Performance Analyzer in Test workspace with realistic data volumes before promoting to Production
- **Refresh Testing**: Execute full data refresh in Test workspace, measure refresh duration (target <10 minutes for Import mode)
- **Load Testing**: Simulate multiple concurrent users accessing Test dashboard to validate performance under load

**Test Data Requirements**:
| Dataset | Dev Workspace | Test Workspace | Production |
|---------|---------------|----------------|------------|
| DimPatient | 500 rows | 50,000 rows | 75,000 rows |
| FactEncounter | 5,000 rows | 2,000,000 rows | 5,000,000 rows |
| Refresh Time Target | <1 minute | <5 minutes | <10 minutes |

### L Pitfall 2: No Rollback Plan for Production Deployments
**Description**: Deploying changes to Production without keeping a backup of the previous version or documented rollback procedure, assuming "it will work because it worked in Test."

**Impact**: When production deployment introduces a critical bug (calculation error, broken drillthrough, RLS misconfiguration), team has no way to quickly restore previous working version. Users experience downtime while developers scramble to debug and fix in Production (high-pressure, error-prone). Example: Friday 4:00 PM deployment goes wrong, but previous version not savedentire weekend spent debugging instead of one-click rollback.

**Prevention**:
- **Pre-Deployment Backup**: Before deploying to Production, download current .pbix file and save with timestamp (e.g., "DailyCensus_v3.2_Backup_20251021.pbix")
- **Deployment Pipeline Rollback**: If using Premium Deployment Pipelines, previous version automatically retaineduse "Deploy to previous stage" to rollback
- **Version Control**: Store .pbix or PBIP in Git repository before each Production deploymentrollback is just a git checkout
- **Rollback Decision Criteria**: Define when to rollback vs. fix-forward:
  - Rollback if: Critical calculation error, data security breach, >5 user-reported issues in first hour
  - Fix-forward if: Minor visual alignment issue, tooltip typo, non-critical feature missing

**Rollback Procedure**:
```
PRODUCTION ROLLBACK PROCEDURE (Max 15 minutes)

Scenario: Production deployment introduced critical bug, need to restore previous version

Steps:
1. Locate backup .pbix file (check Dev workspace, local backups, Git repository)
2. Open backup .pbix in Power BI Desktop
3. Verify data source connection strings point to Production database
4. Publish to Production workspace (overwrite current broken version)
5. Verify report loads correctly in Power BI Service
6. Test critical functionality (slicers, filters, drillthrough)
7. Notify stakeholders: "Issue resolved, previous version restored"
8. Root cause analysis: Why did Test not catch this issue?

Total Time: 10-15 minutes
Downtime: <20 minutes from issue detection to rollback complete
```

### L Pitfall 3: Skipping UAT or Using Only Developer Testing
**Description**: Developers test changes themselves (checking that DAX measures calculate correctly, visuals display without errors) but don't involve actual clinical end-users in user acceptance testing before Production deployment.

**Impact**: Developers miss usability issues, misunderstand business logic, or overlook edge cases that real users encounter. Example: Developer builds "readmission rate" measure but miscalculates denominator (counts all patients instead of discharged patients), producing incorrect rate. Clinicians notice immediately in Production (rate shows 140%, clearly wrong), but issue wasn't caught because developer didn't validate with clinical subject matter expert.

**Prevention**:
- **Involve Real Users**: Schedule UAT sessions with 2-3 representative end-users (clinical staff, analysts, managers)
- **Realistic Scenarios**: Provide specific test scenarios based on actual workflows (daily huddle prep, quality reporting, executive review)
- **Known Test Cases**: Use scenarios with known expected results (e.g., "Patient John Doe should show readmission flag because he was readmitted within 30 days")
- **Usability Feedback**: Ask users "Is this useful? Does it make sense? What's confusing?" (not just "Does it work?")
- **Subject Matter Expert Validation**: For complex clinical calculations (HEDIS, CMS Stars, risk scores), have clinical SME validate logic and results

**UAT Invitation Email**:
```
Subject: UAT Request - Quality Dashboard Update (30 minutes, Oct 22, 2:00 PM)

Hi Dr. Johnson,

We've updated the Quality Metrics Dashboard with new HEDIS measure calculations and
would love your feedback before deploying to production next week.

Could you join a 30-minute UAT session on Tuesday, Oct 22 at 2:00 PM?

What we need from you:
- Review updated HEDIS measures (Breast Cancer Screening, Diabetes HbA1c Control)
- Validate calculations against known test cases (we'll provide patient examples)
- Test dashboard usability with your typical workflow
- Provide feedback on clarity and usefulness

Test workspace link: [URL]
Test scenarios document: [Attached]

Your clinical expertise is invaluable - thank you!
```

### L Pitfall 4: Last-Minute Friday Afternoon Deployments
**Description**: Deploying changes to Production on Friday afternoon (4:00-5:00 PM) or right before long weekends/holidays, leaving no time to monitor for issues or respond to user feedback before team leaves for weekend.

**Impact**: If deployment introduces a critical bug, users are stuck with broken dashboard all weekend. On-call support team may lack Power BI expertise to troubleshoot. Users lose confidence in dashboard reliability. Example: Friday 4:30 PM deployment breaks RLS configurationproviders can see other providers' patients (PHI breach). Issue not discovered until Monday, but damage done.

**Prevention**:
- **Deploy Early in Week**: Tuesday-Wednesday deployments allow time to monitor and fix issues before weekend
- **Deploy Early in Day**: Morning deployments (8:00-10:00 AM) allow full day of monitoring
- **Avoid Fridays/Holidays**: No Production deployments on Fridays, day before holidays, or during critical business periods (month-end close, board meeting week)
- **Deployment Calendar**: Maintain shared calendar with "deployment blackout periods" (critical business periods)
- **Post-Deployment Monitoring**: After Production deployment, monitor for 2-4 hours (watch for user reports, performance issues, data refresh failures)

**Deployment Windows (Best Practices)**:
```
 SAFE DEPLOYMENT WINDOWS:
  - Tuesday-Thursday, 8:00 AM - 12:00 PM (allows full-day monitoring)
  - Low-usage periods (early morning before clinical staff arrive)
  - Post-UAT with sign-off (stakeholders approved changes)

 AVOID DEPLOYMENT WINDOWS:
  - Friday afternoons (no time to fix before weekend)
  - Day before long weekends/holidays
  - During critical business periods (month-end reporting, executive board meetings)
  - Late afternoon/evening (developers offline, limited support)
  - Same day as major system updates (EMR upgrades, network maintenance)
```

## Healthcare Context

### Clinical Workflow Protection
**Zero Tolerance for Daily Huddle Disruption**: Daily 8:00 AM clinical huddles are mission-critical. Census dashboard must load reliably. Deployment failures during huddle time cause providers to skip data review, potentially missing high-risk patients. Schedule Production deployments outside huddle hours (deploy by 7:00 AM or after 9:00 AM).

**Change Timing Considerations**:
- **Avoid month-end**: Financial/quality reporting deadlines (providers rely on dashboards for month-end submission)
- **Avoid flu season peak**: October-March, clinical staff overwhelmedminimize non-critical changes
- **Coordinate with EMR updates**: Don't deploy Power BI changes same week as EMR system updates (too much change at once)

### Validation with Clinical Subject Matter Experts
**Complex Clinical Calculations**: HEDIS measures, CMS Stars, risk stratification models require clinical validation. Developers may misinterpret specifications. Example: "Readmission within 30 days"does this mean 30 calendar days or 30 days from discharge date? Clinical SME clarifies.

**UAT for Clinical Accuracy**: Invite clinicians to UAT not just for usability but for clinical correctness. Provide test cases with known outcomes: "Patient Jane Doe was discharged on Oct 1 and readmitted on Oct 25should she show readmission flag? Yes."

### HIPAA Compliance in Testing
**Test Data PHI Considerations**: Test workspace should NOT use production data with real patient names/PHI unless de-identified or test environment is HIPAA-compliant. Options:
1. **Synthetic Test Data**: Generate fake patient data (preferred for Dev, acceptable for Test)
2. **De-Identified Production Copy**: Remove names, MRNs, addresses before copying to Test database
3. **Compliant Test Environment**: If using real PHI in Test, ensure Test workspace has same access controls as Production (RLS, workspace roles)

**Audit Trail**: Document all Production deployments in change log for HIPAA compliance audits. Include: who deployed, when, what changed, UAT sign-off, rollback plan.

## Learn More

### Official Documentation
- [Power BI deployment pipelines](https://learn.microsoft.com/en-us/power-bi/create-reports/deployment-pipelines-overview) - Microsoft Learn guide to Premium deployment pipelines with automated testing and rollback capabilities
- [Test reports with Performance Analyzer](https://learn.microsoft.com/en-us/power-bi/create-reports/desktop-performance-analyzer) - Using Performance Analyzer to validate performance before and after changes
- [Power BI best practices for testing](https://learn.microsoft.com/en-us/power-bi/guidance/powerbi-implementation-planning-testing-strategy) - Microsoft guidance on testing strategies, UAT processes, and regression testing

### Expert Resources
- [UAT Best Practices for Power BI](https://www.powerbi.tips/2021/11/user-acceptance-testing-for-power-bi/) - PowerBI.Tips article on structuring UAT sessions, creating test plans, and gathering user feedback
- [Power BI Change Management Template](https://github.com/microsoft/powerbi-desktop-samples) - Microsoft sample templates for change notifications, UAT checklists, and deployment procedures
- [Regression Testing with DAX Studio](https://www.sqlbi.com/articles/automating-power-bi-testing-with-dax-studio/) - SQLBI guide to automated regression testing using DAX queries and baseline comparisons

### Video Content
- [Deployment Pipelines and Testing - Guy in a Cube](https://www.youtube.com/watch?v=07t6KBeyEzQ) - Patrick and Adam demonstrate deployment pipeline workflow with UAT and rollback scenarios
- [UAT Process for Enterprise BI - SQLBI](https://www.youtube.com/watch?v=ZdFp8vH6YQg) - Alberto Ferrari on enterprise UAT best practices, stakeholder management, and sign-off procedures
- [Power BI DevOps and Testing - Microsoft](https://www.youtube.com/watch?v=qmJr0B8vJw4) - Official Microsoft overview of DevOps practices including automated testing and CI/CD integration

### Related Topics in This Guide
- [05.1 - Dev/Test/Prod Workspace Strategy](./05.1%20-%20Dev%20Test%20Prod%20Workspace%20Strategy.md) - Three-environment setup essential for proper testing workflow before Production deployment
- [05.3 - Version Control for Power BI](./05.3%20-%20Version%20Control%20for%20Power%20BI.md) - Git integration enables rollback to previous versions and change tracking for audit trail
- [06.3 - Deployment Pipelines & CI-CD](../06%20-%20Governance%20Security%20&%20Deployment/06.3%20-%20Deployment%20Pipelines%20&%20CI-CD.md) - Automated deployment with testing gates and rollback capabilities
- [02.4 - Performance Analyzer Workflow](../02%20-%20Performance%20Optimization%20&%20Query%20Design/02.4%20-%20Performance%20Analyzer%20Workflow.md) - Performance testing before/after changes to validate no regressions

---

*Last updated: October 21, 2025*

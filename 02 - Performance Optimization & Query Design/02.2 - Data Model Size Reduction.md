# 02.2 - Data Model Size Reduction

## Overview

Power BI's VertiPaq engine uses columnar compression to store data in memory, making model size a critical performance factor. A smaller model loads faster into memory, refreshes more quickly, and responds to queries with lower latency. For healthcare analytics with millions of encounter and claims records, strategic data model size reduction can mean the difference between a 2-second report load and a 12-second abandonment. Understanding VertiPaq compression mechanics, data type optimization, and column elimination strategies enables developers to build lean, performant semantic models that meet clinical workflow SLAs.

## Key Principles

- **VertiPaq Loves Patterns**: Columnar compression works best with repeated values (high cardinality = poor compression, low cardinality = excellent compression)removing unique identifier columns can reduce size by 40-60%
- **Data Types Matter Significantly**: Integer compresses better than text, Date compresses better than DateTime, and reducing precision (e.g., Decimal(10,2) instead of Decimal(18,6)) directly reduces storage
- **Only Import What You Need**: Every column consumes memoryeliminate unused columns, audit "just in case" fields, and push aggregations to source when possible
- **Aggregations are Force Multipliers**: Pre-aggregated tables (daily/monthly grain vs transaction grain) can reduce query volume by 90%+ while maintaining user experience for high-level analysis
- **Monitor and Measure**: Use DAX Studio's VertiPaq Analyzer to identify size offendersmeasure before/after optimization to quantify improvements

## Practical Examples

### Example 1: Removing High-Cardinality Columns

**Scenario:** FactEncounter table has 2.5 million rows. Several columns are imported "just in case" but rarely used or have poor compression characteristics.

**Audit columns in DAX Studio VertiPaq Analyzer:**

```
Column Name                     | Cardinality | Size (MB) | Compression Ratio
--------------------------------|-------------|-----------|------------------
EncounterKey (surrogate)        | 2,500,000   | 18.2 MB   | 1.2:1 (poor)
EncounterGUID (source system)   | 2,500,000   | 42.7 MB   | 0.8:1 (terrible)
EncounterDate                   | 1,095       | 0.8 MB    | 280:1 (excellent)
PatientKey                      | 125,000     | 3.2 MB    | 12:1 (good)
ProviderKey                     | 450         | 0.2 MB    | 450:1 (excellent)
FacilityKey                     | 12          | 0.1 MB    | 1200:1 (excellent)
EncounterType                   | 8           | 0.1 MB    | 1500:1 (excellent)
EncounterAmount                 | 87,500      | 8.4 MB    | 4.5:1 (moderate)
EncounterNotes (text)           | 1,850,000   | 127.3 MB  | 0.6:1 (terrible)
CreatedBy (username text)       | 42          | 0.3 MB    | 340:1 (excellent)
CreatedDateTime                 | 2,487,000   | 24.1 MB   | 1.1:1 (poor)
ModifiedDateTime                | 2,499,000   | 24.8 MB   | 1.1:1 (poor)
--------------------------------|-------------|-----------|------------------
TOTAL                           |             | 250.2 MB  |
```

**Columns to remove:**

1. **EncounterGUID** (42.7 MB): Source system identifier, not used in any relationships or visuals
   - Keep EncounterKey (surrogate) for relationships
   - GUID is uniquely high-cardinality (one per row), compresses terribly
   - **Savings: 42.7 MB (17% of table size)**

2. **EncounterNotes** (127.3 MB): Free-text clinical notes, almost unique per row
   - Used in only 1 of 15 reports (detailed drill-through)
   - Solution: Keep in source database, use DirectQuery for drill-through page
   - **Savings: 127.3 MB (51% of table size)**

3. **CreatedDateTime & ModifiedDateTime** (48.9 MB combined): Audit timestamps with second-level precision
   - Replace with CreatedDate (Date type) for reporting purposes
   - Reduces 2.5M unique datetime values to ~1,095 unique dates
   - **Savings: 48.0 MB (19% of table size after keeping CreatedDate at 0.9 MB)**

**Implementation in Power Query:**

```powerquery
// FactEncounter query - Remove unnecessary columns
let
    Source = Sql.Database("ABCDW-Server", "ABCDW"),
    StarSchema_FactEncounter = Source{[Schema="StarSchema", Item="FactEncounter"]}[Data],

    // Remove high-cardinality columns not used in model
    RemovedColumns = Table.RemoveColumns(
        StarSchema_FactEncounter,
        {
            "EncounterGUID",        // Not used in relationships or visuals
            "EncounterNotes",       // Keep in database, use DirectQuery for drill-through
            "CreatedDateTime",      // Replacing with CreatedDate below
            "ModifiedDateTime"      // Not used in any reports
        }
    ),

    // Add Date-only version of CreatedDateTime for audit reporting
    AddedCreatedDate = Table.AddColumn(
        RemovedColumns,
        "CreatedDate",
        each Date.From([CreatedDateTime]),
        type date
    ),

    // Remove the original CreatedDateTime after extracting Date
    FinalColumns = Table.RemoveColumns(AddedCreatedDate, {"CreatedDateTime"})
in
    FinalColumns
```

**Results after removing 4 columns:**

```
BEFORE: 250.2 MB
AFTER:  32.2 MB
SAVINGS: 218 MB (87% reduction)
```

**Impact on <5s SLA:**
- Smaller model loads faster into memory (6s ’ 1.2s initial load)
- Refresh time reduced (18 min ’ 4 min for 2.5M rows)
- Query performance improved (less data to scan)

**Why this works:**
- VertiPaq compresses repeated values, not unique values
- GUID and free-text columns have nearly 100% unique values (worst case for compression)
- DateTime with seconds precision creates millions of unique values
- Date with day precision creates ~1,000 unique values (compresses 1000x better)

### Example 2: Optimizing Data Types for Compression

**Scenario:** DimPatient dimension has 250,000 patients. Several numeric and text columns use suboptimal data types.

**Before optimization:**

```powerquery
// DimPatient query - Suboptimal data types
let
    Source = Sql.Database("ABCDW-Server", "ABCDW"),
    DimPatient = Source{[Schema="StarSchema", Item="DimPatient"]}[Data],

    // Data types inherited from SQL Server (often suboptimal for VertiPaq)
    // PatientKey: Int64 (8 bytes per row)
    // SourcePatientID: Text (variable length, 12-15 characters)
    // PatientAgeGroup: Text ("Pediatric (0-17)", "Adult (18-64)", "Senior (65+)")
    // IsActive: Text ("True", "False")
    // ZipCode: Text ("12345" or "12345-6789")
in
    DimPatient
```

**Data type analysis:**

| Column | Current Type | Current Size | Cardinality | Optimal Type | Optimized Size | Savings |
|--------|-------------|--------------|-------------|--------------|----------------|---------|
| PatientKey | Int64 | 1.8 MB | 250,000 | Int32 | 0.9 MB | 0.9 MB |
| SourcePatientID | Text | 3.2 MB | 250,000 | Keep Text | 3.2 MB | 0 MB |
| PatientAgeGroup | Text | 0.4 MB | 3 | Keep Text | 0.4 MB | 0 MB |
| IsActive | Text | 0.6 MB | 2 | Boolean | 0.1 MB | 0.5 MB |
| ZipCode | Text | 1.8 MB | 12,000 | Text (keep) | 1.8 MB | 0 MB |
| DateOfBirth | DateTime | 2.1 MB | 28,000 | Date | 0.3 MB | 1.8 MB |

**After optimization:**

```powerquery
// DimPatient query - Optimized data types
let
    Source = Sql.Database("ABCDW-Server", "ABCDW"),
    DimPatient = Source{[Schema="StarSchema", Item="DimPatient"]}[Data],

    // Optimize PatientKey: Int64 ’ Int32 (250K patients fits in 2.1 billion Int32 max)
    ChangedKeyType = Table.TransformColumnTypes(
        DimPatient,
        {{"PatientKey", Int32.Type}}
    ),

    // Optimize IsActive: Text ’ Boolean
    ChangedIsActiveType = Table.TransformColumnTypes(
        ChangedKeyType,
        {{"IsActive", type logical}}
    ),

    // Optimize DateOfBirth: DateTime ’ Date (no time component needed)
    ChangedDateType = Table.TransformColumnTypes(
        ChangedIsActiveType,
        {{"DateOfBirth", type date}}
    )
in
    ChangedDateType
```

**Results:**

```
BEFORE: 9.9 MB
AFTER:  6.7 MB
SAVINGS: 3.2 MB (32% reduction)
```

**Data type decision framework:**

```
Integer Keys:
  < 32,767 rows ’ Int16 (2 bytes)
  < 2.1 billion rows ’ Int32 (4 bytes)
  > 2.1 billion rows ’ Int64 (8 bytes)

Dates/Times:
  Day precision needed ’ Date (no time component)
  Time component needed ’ DateTime
  NEVER use Text for dates (breaks time intelligence, poor compression)

Booleans:
  "True"/"False" text ’ Boolean type (6x size reduction)
  "Y"/"N" text ’ Boolean type
  1/0 integer ’ Boolean type (or keep Integer for sum aggregations)

Text:
  Keep text when: truly variable-length content (names, addresses)
  Consider integer: when lookup table possible (replace "Blue"/"Red"/"Green" with 1/2/3)

Decimals:
  Currency ’ Decimal(10,2) or Fixed Decimal
  Percentages ’ Decimal(5,4) or Float
  AVOID Decimal(18,6) unless precision required (larger storage)
```

### Example 3: Using Aggregation Tables for Large Fact Tables

**Scenario:** FactEncounter has 2.5 million rows covering 3 years of daily encounter data. Executive dashboards show monthly/quarterly trends but still query row-level data (slow).

**Problem:**

```dax
// Executive dashboard measure
Total Encounters by Month =
CALCULATE(
    COUNTROWS(FactEncounter),
    // This scans 2.5 million rows to count encounters per month
    // Even though result is only 36 monthly aggregates
)

// Performance: 2,847 ms (slow for simple visual)
```

**Solution: Create aggregation table at monthly grain**

**Step 1: Create FactEncounterMonthly in SQL or Power Query**

```sql
-- ABCDW.StarSchema.FactEncounterMonthly (SQL view)
CREATE VIEW ABCDW.StarSchema.FactEncounterMonthly AS
SELECT
    -- Date grain: First day of month
    DATEADD(MONTH, DATEDIFF(MONTH, 0, EncounterDate), 0) AS MonthStartDate,

    -- Dimension keys (preserved for relationships)
    ProviderKey,
    FacilityKey,
    EncounterTypeKey,

    -- Pre-aggregated measures
    COUNT(*) AS TotalEncounters,
    COUNT(DISTINCT PatientKey) AS UniquePatients,
    SUM(EncounterAmount) AS TotalRevenue,
    AVG(EncounterDuration) AS AvgEncounterDuration

FROM ABCDW.StarSchema.FactEncounter
GROUP BY
    DATEADD(MONTH, DATEDIFF(MONTH, 0, EncounterDate), 0),
    ProviderKey,
    FacilityKey,
    EncounterTypeKey;
```

**Step 2: Import aggregation table in Power BI**

```powerquery
// FactEncounterMonthly query
let
    Source = Sql.Database("ABCDW-Server", "ABCDW"),
    FactEncounterMonthly = Source{[Schema="StarSchema", Item="FactEncounterMonthly"]}[Data]
in
    FactEncounterMonthly
```

**Row count comparison:**

```
FactEncounter (daily):        2,500,000 rows ’ 250 MB
FactEncounterMonthly:              1,260 rows ’ 0.4 MB
  (36 months × 450 providers × ~12 facilities × ~8 encounter types with some sparse combinations)

REDUCTION: 99.95% fewer rows, 625x smaller
```

**Step 3: Configure automatic aggregation (Premium feature)**

```
Power BI Desktop ’ Modeling tab ’ Manage Aggregations:

FactEncounterMonthly table:
  TotalEncounters ’ Sum of FactEncounter[EncounterKey] (COUNT)
  UniquePatients ’ DistinctCount of FactEncounter[PatientKey]
  TotalRevenue ’ Sum of FactEncounter[EncounterAmount]
  AvgEncounterDuration ’ Average of FactEncounter[EncounterDuration]

Precedence: FactEncounterMonthly (higher than FactEncounter)
```

**Step 4: Power BI automatically uses aggregation when appropriate**

```dax
// Same measure as before - no changes needed
Total Encounters by Month =
CALCULATE(
    COUNTROWS(FactEncounter)
)

// Power BI engine now automatically queries FactEncounterMonthly
// when visual is filtered to monthly grain
// Performance: 89 ms (32x faster)
```

**When aggregation is used vs. not used:**

```
Executive dashboard (monthly grain):
  Visual: Column chart by Month
   Power BI uses: FactEncounterMonthly (1,260 rows)
  Performance: 89 ms

Detailed encounter drill-through (daily grain):
  Visual: Table showing individual encounters
   Power BI uses: FactEncounter (2.5M rows)
  Performance: 1,240 ms (acceptable for detail view)

BENEFIT: 95% of visuals hit aggregation table, 5% query detail when needed
```

**Non-Premium alternative: Manual aggregation table with separate measures**

```dax
// Create separate measures for monthly aggregation table
// (No automatic aggregation without Premium)

Total Encounters (Monthly) =
SUM(FactEncounterMonthly[TotalEncounters])

Total Encounters (Daily) =
COUNTROWS(FactEncounter)

// Developer manually chooses which measure to use in each visual
// Executive dashboard uses "Total Encounters (Monthly)"
// Detail reports use "Total Encounters (Daily)"
```

## Common Pitfalls

### Pitfall 1: Importing Every Column "Just in Case"

Importing all source table columns without auditing usage, assuming "storage is cheap" or "we might need it later," leading to bloated models with 40-60% unused columns.

**Impact**: Unnecessary model size (slower load times), wasted refresh time (calculating/compressing unused columns), increased memory consumption (every column loaded even if not used), slower queries (more columns to scan), maintenance confusion (developers unsure which columns are actually used).

**Example mistake:**

```powerquery
// Power Query - importing everything from source
let
    Source = Sql.Database("ABCDW-Server", "ABCDW"),
    FactEncounter = Source{[Schema="StarSchema", Item="FactEncounter"]}[Data]
    // 45 columns imported, only 18 used in any report
in
    FactEncounter
```

**Audit with DAX Studio:**

```
VertiPaq Analyzer ’ Columns tab ’ Sort by Size descending

Top 10 columns by size:
1. EncounterNotes - 127 MB - Used in: 0 reports
2. EncounterGUID - 42 MB - Used in: 0 reports
3. ModifiedDateTime - 24 MB - Used in: 0 reports
4. CreatedDateTime - 24 MB - Used in: 1 report (audit log)
5. SourceSystemRawJSON - 38 MB - Used in: 0 reports
...

TOTAL UNUSED: 287 MB out of 420 MB (68% waste)
```

**Fix: Explicit column selection**

```powerquery
// Power Query - only import what you need
let
    Source = Sql.Database("ABCDW-Server", "ABCDW"),
    AllColumns = Source{[Schema="StarSchema", Item="FactEncounter"]}[Data],

    // Explicitly select only used columns
    SelectedColumns = Table.SelectColumns(
        AllColumns,
        {
            "EncounterKey",
            "EncounterDate",
            "PatientKey",
            "ProviderKey",
            "FacilityKey",
            "EncounterTypeKey",
            "EncounterAmount",
            "EncounterDuration",
            "IsReadmission"
            // Only 9 of 45 columns actually used
        }
    )
in
    SelectedColumns
```

**Results:**

```
BEFORE (45 columns): 420 MB, 18 min refresh
AFTER (9 columns):   133 MB, 6 min refresh
SAVINGS: 287 MB (68%), 12 min refresh time
```

**Best practice workflow:**

1. Import minimal set initially (keys + obvious measures)
2. Add columns as needed when building visuals
3. Quarterly audit: Run VertiPaq Analyzer, check unused columns
4. Document column purpose in M query comments
5. Remove unused columns before production deployment

### Pitfall 2: Using Text Data Types for Everything

Allowing Power Query to auto-detect data types or inheriting suboptimal types from source (e.g., Text for Boolean, Text for Date, Int64 for small integers), missing compression opportunities.

**Impact**: 3-10x larger model size than necessary, poor compression (text doesn't compress as well as integer/boolean), breaks functionality (text dates don't work with time intelligence), slower queries (text comparison slower than integer/boolean), wasted refresh time (unnecessary type conversion overhead).

**Example mistake:**

```powerquery
// DimPatient - auto-detected types from CSV source
let
    Source = Csv.Document(File.Contents("C:\Data\Patients.csv")),
    PromotedHeaders = Table.PromoteHeaders(Source),

    // Auto-detected types (all Text by default):
    // PatientKey: "123456" (text)
    // DateOfBirth: "1985-03-15 00:00:00" (text)
    // IsActive: "True" (text)
    // PatientAgeGroup: "Adult (18-64)" (text)

    AutoDetectedTypes = Table.TransformColumnTypes(
        PromotedHeaders,
        {
            {"PatientKey", type text},         // WRONG: Should be Int32
            {"DateOfBirth", type text},        // WRONG: Should be Date
            {"IsActive", type text},           // WRONG: Should be Boolean
            {"PatientAgeGroup", type text}     // OK: Truly text (3 categories)
        }
    )
in
    AutoDetectedTypes

// Model size: 14.2 MB for 250K patients
```

**Fix: Explicit optimal types**

```powerquery
// DimPatient - optimized data types
let
    Source = Csv.Document(File.Contents("C:\Data\Patients.csv")),
    PromotedHeaders = Table.PromoteHeaders(Source),

    // Explicitly set optimal data types
    OptimizedTypes = Table.TransformColumnTypes(
        PromotedHeaders,
        {
            {"PatientKey", Int32.Type},             //  Text ’ Int32 (4 bytes)
            {"DateOfBirth", type date},             //  Text ’ Date (compressed)
            {"IsActive", type logical},             //  Text ’ Boolean (1 bit)
            {"PatientAgeGroup", type text}          //  Text (appropriate: 3 values)
        }
    )
in
    OptimizedTypes

// Model size: 6.7 MB for 250K patients (53% reduction)
```

**Type optimization checklist:**

- [ ] Integer keys use smallest type (Int16/Int32/Int64)
- [ ] Date columns use Date not DateTime (unless time needed)
- [ ] Boolean flags use Boolean not Text ("True"/"False")
- [ ] Numeric measures use appropriate Decimal precision
- [ ] Text reserved for truly variable content (names, descriptions)
- [ ] No text dates (breaks time intelligence)

### Pitfall 3: Not Monitoring Model Size After Changes

Adding columns, measures, and calculated columns over time without tracking model size growth, leading to gradual performance degradation ("it used to be fast").

**Impact**: Model grows from 200 MB to 1.2 GB over 6 months without noticing, refresh time increases from 3 min to 25 min, report load time degrades from <2s to 8s (users abandon), root cause unclear (many small changes vs. one big problem), expensive remediation (hard to identify which changes caused bloat).

**Example scenario:**

```
Initial deployment (Month 0):
  Model size: 187 MB
  Refresh time: 3 min
  Report load: 1.2s
   Users happy

After 3 months:
  Model size: 542 MB (190% increase)
  Refresh time: 12 min
  Report load: 4.8s
   Users starting to complain

After 6 months:
  Model size: 1.18 GB (531% increase)
  Refresh time: 28 min
  Report load: 11.2s
   Users abandoning reports, project at risk
```

**What happened? Gradual column creep:**

```
Month 1: Added 3 calculated columns for new slicers (+45 MB)
Month 2: Imported EncounterNotes for drill-through feature (+127 MB)
Month 3: Added 5 "just in case" columns from new source table (+82 MB)
Month 4: Created 8 calculated columns for convenience groupings (+118 MB)
Month 5: Imported historical data back 2 more years (+380 MB)
Month 6: Added patient address fields (not used yet) (+35 MB)

TOTAL GROWTH: +787 MB (420% increase)
ACTUALLY USED: ~210 MB worth of genuinely needed additions
WASTE: ~577 MB (73% of growth was unnecessary)
```

**Solution: Establish monitoring cadence**

**Monthly: DAX Studio VertiPaq Analyzer report**

```
Export VertiPaq Analyzer metrics:
  - Total model size
  - Top 20 columns by size
  - Top 10 tables by size
  - Cardinality trends
  - Compression ratios

Track in Excel dashboard:
  Month | Model Size | ” from Last Month | Refresh Time | Top Size Offenders
  ------|------------|-------------------|--------------|-------------------
  Jan   | 187 MB     | -                 | 3 min        | FactEncounter (145 MB)
  Feb   | 232 MB     | +45 MB (+24%)     | 5 min        | FactEncounter (168 MB)
  Mar   | 359 MB     | +127 MB (+55%)    | 9 min        | EncounterNotes (127 MB)   ALERT!
```

**Alerting thresholds:**

- Model size increases >20% month-over-month ’ investigate
- Refresh time increases >30% ’ investigate
- Any single column >50 MB ’ review necessity
- Model approaching 1 GB ’ urgent optimization needed (1.5 GB Import mode limit)

**Quarterly: Full model health review**

1. Run VertiPaq Analyzer and Best Practice Analyzer (Tabular Editor)
2. Audit all columns: used in how many visuals? Remove unused.
3. Review data types: any suboptimal types? Re-optimize.
4. Check calculated columns: can any be replaced with measures or SQL calculations?
5. Consider aggregation tables for large fact tables
6. Document findings and remediation plan

**Proactive development workflow:**

```
Before adding any new column to model:
1. Check current model size in VertiPaq Analyzer
2. Add column in Dev workspace
3. Refresh and measure new model size
4. If increase >5%, justify:
   - Is this column used in visuals? (not "might be someday")
   - Can we calculate in SQL instead?
   - Can we use DirectQuery for this specific column?
   - Is data type optimized?
5. Document in change log: "Added PatientAgeGroup column (+2.1 MB, used in 3 slicers)"
6. Proceed to Test workspace only if justified
```

## Healthcare Context

### Large Encounter and Claims Tables

Healthcare fact tables grow quickly (millions of rows annually):

| Data Type | Annual Volume (Typical) | 3-Year Volume | Row Size | Model Size (3yr) |
|-----------|-------------------------|---------------|----------|------------------|
| Encounters | 800K-1.2M | 2.4M-3.6M | ~120 bytes | 288-432 MB |
| Claims | 1.5M-2.5M | 4.5M-7.5M | ~95 bytes | 427-712 MB |
| Lab Results | 3M-5M | 9M-15M | ~85 bytes | 765-1,275 MB |
| Medications | 2M-3.5M | 6M-10.5M | ~75 bytes | 450-787 MB |

**Combined fact tables without optimization: 1.9-3.2 GB (exceeds Import mode 1.5 GB limit for many organizations)**

**Optimization strategies:**

1. **Column elimination**: Remove clinical notes, audit fields, source system IDs (40-60% reduction)
2. **Data type optimization**: Use smallest integer types, Date instead of DateTime (20-30% reduction)
3. **Aggregation tables**: Monthly/quarterly aggregates for executive dashboards (95% row reduction)
4. **Historical partitioning**: Import last 12 months, DirectQuery for older (or separate "Historical" model)

**Result after optimization: 450-650 MB (well within limits, <5s load time)**

### HIPAA and Compliance Considerations

**PHI in model size:**

- Patient names, addresses, notes = PHI (high cardinality, large text fields)
- Strategy: Keep minimal PHI in Import mode model
- Detailed PHI: DirectQuery composite model for drill-through only
- Audit timestamps: Store Date (not DateTime) unless second-precision required for compliance

**Data retention policies:**

- Many healthcare organizations: 7-year retention for claims, 3-year for operational reporting
- Don't import all 7 years into Power BI model (use SQL for historical queries)
- Power BI model: Last 2-3 years for trending (balance size vs. analytical need)

### Mobile and Field Access

Smaller models sync faster to Power BI Mobile app:

| Model Size | Mobile Sync Time (4G LTE) | User Experience |
|------------|---------------------------|-----------------|
| <100 MB | 8-15 seconds | Excellent |
| 100-300 MB | 20-45 seconds | Good |
| 300-600 MB | 1-2 minutes | Acceptable |
| 600 MB-1 GB | 2-4 minutes | Poor (users give up) |
| >1 GB | 5-10+ minutes | Unacceptable |

**Provider field access scenario:**
- Home health nurse checking patient census on tablet
- Model size <300 MB = 30 second sync = feasible workflow
- Model size >600 MB = 3 minute sync = nurse abandons mobile app

**Recommendation:** Target <300 MB for models intended for mobile field access.

## Learn More

### Official Documentation

- [Reduce Data Model Size](https://learn.microsoft.com/en-us/power-bi/guidance/import-modeling-data-reduction) - Microsoft's official guidance on model optimization
- [Aggregations in Power BI](https://learn.microsoft.com/en-us/power-bi/transform-model/aggregations-advanced) - Premium aggregation tables configuration
- [Data Types in Power BI](https://learn.microsoft.com/en-us/power-bi/connect-data/desktop-data-types) - Understanding VertiPaq data type storage

### Expert Resources

- [SQLBI - Optimizing VertiPaq Compression](https://www.sqlbi.com/articles/optimizing-vertipaq-compression/) - Marco Russo's deep dive into compression mechanics
- [SQLBI - Analyzing VertiPaq with DAX Studio](https://www.sqlbi.com/tools/dax-studio/) - Using VertiPaq Analyzer for model size audits
- [PowerBI.Tips - Model Size Optimization](https://powerbi.tips/2020/06/model-size-reduction/) - Practical optimization techniques

### Video Content

- [Guy in a Cube - Reduce Model Size](https://www.youtube.com/c/GuyinaCube) - Patrick and Adam demonstrate size reduction strategies
- [SQLBI - VertiPaq Deep Dive](https://www.sqlbi.com/tv/vertipaq-engine-in-power-bi/) - Understanding columnar compression internals

### Related Topics

- [02.1 - Query Folding & Import vs DirectQuery](./02.1%20-%20Query%20Folding%20&%20Import%20vs%20DirectQuery.md) - Choosing Import mode enables size optimization
- [01.4 - Calculated Columns vs Measures](../01%20-%20Data%20Architecture%20&%20Semantic%20Modeling/01.4%20-%20Calculated%20Columns%20vs%20Measures.md) - Calculated columns increase model size
- [02.3 - Incremental Refresh Strategies](./02.3%20-%20Incremental%20Refresh%20Strategies.md) - Managing historical data volume
- [05.2 - External Tools - Tabular Editor & DAX Studio](../05%20-%20Development%20Workflow%20&%20Version%20Control/05.2%20-%20External%20Tools%20-%20Tabular%20Editor%20&%20DAX%20Studio.md) - Using VertiPaq Analyzer for size monitoring

---

*Last updated: October 21, 2025*
